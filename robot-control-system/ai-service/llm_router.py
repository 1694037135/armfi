"""
LLM å¤šæ¨¡å‹è·¯ç”±ç³»ç»Ÿ
å®ç°æ™ºèƒ½è·¯ç”±ï¼Œæ ¹æ®ä»»åŠ¡ç±»å‹è°ƒç”¨ä¸åŒçš„æ¨¡å‹
"""
import logging
import httpx
import json
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

class LLMRouter:
    """
    å¤šæ¨¡å‹è·¯ç”±ç®¡ç†å™¨
    
    æ¨¡å‹åˆ†å·¥ï¼š
    - MODEL_FILTER: å¿«é€Ÿæ„å›¾åˆ†ç±»ï¼ˆchat/work/visionï¼‰- Doubao-lite
    - MODEL_DECISION: å·¥ä½œæŒ‡ä»¤å†³ç­– - DeepSeek
    - MODEL_VISION: è§†è§‰ç†è§£
    - MODEL_EMBEDDING: å‘é‡æ£€ç´¢/ä¸Šä¸‹æ–‡ç†è§£
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_key = config.get("GEMINI_API_KEY")
        self.base_url = config.get("LLM_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3")
        self.api_url = f"{self.base_url}/chat/completions"
        
        # å››ä¸ªæ¨¡å‹ ID
        self.model_filter = config.get("MODEL_FILTER")      # æ„å›¾åˆ†ç±»
        self.model_decision = config.get("MODEL_DECISION")  # DeepSeek å†³ç­–
        self.model_vision = config.get("MODEL_VISION")      # è§†è§‰ç†è§£
        self.model_embedding = config.get("MODEL_EMBEDDING") # å‘é‡æ£€ç´¢
        
        # ä»£ç†é…ç½®
        self.proxy_url = config.get("HTTP_PROXY")
        
        logger.info(f"ğŸš€ LLMè·¯ç”±å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - æ„å›¾åˆ†ç±»: {self.model_filter or 'N/A'}")
        logger.info(f"  - å†³ç­–å¤§è„‘: {self.model_decision or 'N/A'}")
        logger.info(f"  - è§†è§‰ç†è§£: {self.model_vision or 'N/A'}")
        logger.info(f"  - å‘é‡æ£€ç´¢: {self.model_embedding or 'N/A'}")
    
    async def _call_llm(
        self, 
        model: str, 
        messages: list, 
        temperature: float = 0.7,
        timeout: float = 10.0
    ) -> Optional[str]:
        """
        è°ƒç”¨ LLM API
        
        Args:
            model: æ¨¡å‹ ID
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            LLM å“åº”å†…å®¹ï¼Œå¤±è´¥è¿”å› None
        """
        if not model:
            logger.error("æ¨¡å‹ ID ä¸ºç©ºï¼Œæ— æ³•è°ƒç”¨")
            return None
            
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": False
        }
        
        # é…ç½®ä»£ç†
        timeout_obj = httpx.Timeout(timeout, connect=5.0)
        if self.proxy_url:
            client_args = {
                "mounts": {
                    "http://": httpx.HTTPTransport(proxy=self.proxy_url),
                    "https://": httpx.HTTPTransport(proxy=self.proxy_url)
                }
            }
        else:
            client_args = {"trust_env": False}
        
        try:
            async with httpx.AsyncClient(timeout=timeout_obj, **client_args) as client:
                resp = await client.post(
                    self.api_url,
                    json=payload,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {self.api_key}"
                    }
                )
                
                if resp.status_code == 200:
                    data = resp.json()
                    content = data["choices"][0]["message"]["content"]
                    return content.strip()
                else:
                    logger.error(f"LLM API é”™è¯¯: {resp.status_code} - {resp.text}")
                    return None
                    
        except httpx.TimeoutException:
            logger.error(f"LLM è°ƒç”¨è¶…æ—¶ (model={model}, timeout={timeout}s)")
            return None
        except Exception as e:
            logger.error(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    async def classify_intent(self, user_message: str) -> str:
        """
        ã€Step 1ã€‘æ„å›¾åˆ†ç±»
        
        ä½¿ç”¨è½»é‡æ¨¡å‹ (MODEL_FILTER) å¿«é€Ÿåˆ¤æ–­ç”¨æˆ·æ„å›¾
        
        Args:
            user_message: ç”¨æˆ·è¾“å…¥
        
        Returns:
            æ„å›¾ç±»å‹: 'chat' | 'work' | 'vision'
        """
        # å…³é”®è¯åˆ¤æ–­ä½œä¸º fallback
        def keyword_classify(msg: str) -> str:
            work_keywords = ["ç§»åŠ¨", "å¤ä½", "è½¬åˆ°", "å…³èŠ‚", "æ§åˆ¶", "æ‹¿", "æ¡", "æŠ“", "æŒ¥æ‰‹", "ç‚¹å¤´", "æ—‹è½¬"]
            vision_keywords = ["çœ‹", "è¯†åˆ«", "æ£€æµ‹", "æ‘„åƒå¤´", "è§†è§‰", "å›¾åƒ", "æ‹ç…§"]
            
            result_intent = "chat"
            if any(kw in msg for kw in vision_keywords):
                result_intent = "vision"
            elif any(kw in msg for kw in work_keywords):
                result_intent = "work"
            
            logger.info(f"ğŸ”‘ [Keyword] Classification: {result_intent}")
            return result_intent
        
        # [Fast Path] ä¼˜å…ˆä½¿ç”¨å…³é”®è¯å¿«é€Ÿåˆ†ç±»ï¼Œé¿å¼€é«˜å»¶è¿Ÿ LLM
        # å¦‚æœå…³é”®è¯æ˜ç¡®æŒ‡ç¤ºäº† work æˆ– visionï¼Œç›´æ¥è¿”å›ï¼Œä¸å†è°ƒç”¨ LLM
        fast_intent = keyword_classify(user_message)
        if fast_intent in ["work", "vision"]:
            logger.info(f"âš¡ [Fast Path] Intent detected: {fast_intent} (Skipping LLM)")
            return fast_intent
        
        if not self.model_filter:
            # å¦‚æœæ²¡æœ‰é…ç½®åˆ†ç±»æ¨¡å‹ï¼Œä½¿ç”¨å…³é”®è¯åˆ¤æ–­
            logger.warning("æœªé…ç½® MODEL_FILTERï¼Œä½¿ç”¨å…³é”®è¯åˆ¤æ–­")
            return keyword_classify(user_message)
        
        # ä½¿ç”¨ LLM åˆ†ç±»
        classify_prompt = f"""è¯·åˆ¤æ–­ç”¨æˆ·æ¶ˆæ¯çš„æ„å›¾ç±»å‹ï¼Œåªè¿”å›ä¸€ä¸ªè¯ï¼šchatã€work æˆ– visionã€‚

åˆ†ç±»æ ‡å‡†ï¼š
- chat: æ™®é€šèŠå¤©ã€é—®å€™ã€æé—®ç­‰
- work: æ§åˆ¶æœºæ¢°è‡‚çš„æŒ‡ä»¤ï¼ˆç§»åŠ¨ã€å¤ä½ã€æŠ“å–ç­‰ï¼‰
- vision: éœ€è¦è§†è§‰è¯†åˆ«çš„ä»»åŠ¡ï¼ˆçœ‹ã€è¯†åˆ«ç‰©ä½“ç­‰ï¼‰

ç”¨æˆ·æ¶ˆæ¯: "{user_message}"

æ„å›¾ç±»å‹:"""
        
        messages = [{"role": "user", "content": classify_prompt}]
        
        logger.info(f"ğŸ” [Step 1] æ„å›¾åˆ†ç±»ä¸­...")
        result = await self._call_llm(
            model=self.model_filter,
            messages=messages,
            temperature=0.3,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
            timeout=30.0  # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥é€‚åº”æ…¢é€Ÿ API (å®æµ‹ >13s)
        )
        
        logger.info(f"ğŸ” [Step 1] Classification Result: {result}")
        
        if result:
            intent = result.lower().strip()
            if intent in ["chat", "work", "vision"]:
                logger.info(f"âœ… æ„å›¾åˆ†ç±»: {intent}")
                return intent
            else:
                logger.warning(f"æ„å›¾åˆ†ç±»ç»“æœå¼‚å¸¸: {result}ï¼Œä½¿ç”¨å…³é”®è¯åˆ¤æ–­")
                return keyword_classify(user_message)
        else:
            # LLM è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åˆ¤æ–­
            logger.warning("æ„å›¾åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åˆ¤æ–­")
            return keyword_classify(user_message)
    
    async def handle_chat(self, user_message: str) -> Dict[str, Any]:
        """
        ã€èŠå¤©æ¨¡å¼ã€‘å¿«é€Ÿå“åº”
        
        å¯¹äºç®€å•èŠå¤©ï¼Œå¯ä»¥ä½¿ç”¨é¢„è®¾å›å¤æˆ–è½»é‡æ¨¡å‹
        """
        logger.info("ğŸ’¬ [Chat Mode] å¤„ç†èŠå¤©æ¶ˆæ¯...")
        
        # é¢„è®¾å›å¤ï¼ˆå¿«é€Ÿå“åº”ï¼‰
        quick_responses = {
            "ä½ å¥½": "ä½ å¥½å‘€ï¼æˆ‘æ˜¯æœºæ¢°è‡‚åŠ©æ‰‹Zeroã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ",
            "ä»‹ç»ä¸€ä¸‹è‡ªå·±": "ä½ å¥½ï¼æˆ‘æ˜¯Zeroæœºæ¢°è‡‚åŠ©æ‰‹ï¼Œä¸€ä¸ªç”±6ä¸ªå…³èŠ‚ç»„æˆçš„æ™ºèƒ½æœºæ¢°è‡‚ã€‚æˆ‘å¯ä»¥æ‰§è¡Œå„ç§ç²¾ç¡®çš„æ§åˆ¶ä»»åŠ¡ï¼Œæ¯”å¦‚ç§»åŠ¨åˆ°æŒ‡å®šä½ç½®ã€è°ƒæ•´å…³èŠ‚è§’åº¦ã€æ‰§è¡Œé¢„è®¾åŠ¨ä½œç­‰ã€‚æœ‰ä»€ä¹ˆéœ€è¦æˆ‘å¸®å¿™çš„å—ï¼Ÿ",
            "ä½ èƒ½åšä»€ä¹ˆ": "æˆ‘å¯ä»¥å¸®ä½ æ§åˆ¶æœºæ¢°è‡‚ï¼æ¯”å¦‚ç§»åŠ¨åˆ°æŒ‡å®šä½ç½®ã€è°ƒæ•´å…³èŠ‚è§’åº¦ã€æ‰§è¡Œé¢„è®¾åŠ¨ä½œï¼ˆå¤ä½ã€å‘å·¦ã€å‘å³ç­‰ï¼‰ï¼Œè¿˜å¯ä»¥æ‰§è¡ŒæŒ¥æ‰‹ã€ç‚¹å¤´ç­‰è¡¨æ¼”åŠ¨ä½œã€‚å‘Šè¯‰æˆ‘ä½ æƒ³è®©æˆ‘åšä»€ä¹ˆå§ï¼",
            "è°¢è°¢": "ä¸å®¢æ°”ï¼éšæ—¶ä¸ºä½ æœåŠ¡ï¼ğŸ˜Š"
        }
        
        # æ£€æŸ¥é¢„è®¾å›å¤
        for key, response in quick_responses.items():
            if key in user_message:
                logger.info(f"âœ… ä½¿ç”¨é¢„è®¾å›å¤")
                return {
                    "success": True,
                    "mode": "chat",
                    "response": response,
                    "fast": True  # æ ‡è®°ä¸ºå¿«é€Ÿå“åº”
                }
        
        # ä½¿ç”¨è½»é‡æ¨¡å‹ï¼ˆMODEL_FILTERï¼‰å¤„ç†èŠå¤©
        chat_prompt = f"""ä½ æ˜¯æœºæ¢°è‡‚åŠ©æ‰‹Zeroï¼Œè¯·ç”¨å‹å¥½çš„è¯­æ°”å›å¤ç”¨æˆ·ã€‚ä¿æŒç®€æ´ã€‚

ç”¨æˆ·: {user_message}

å›å¤:"""
        
        messages = [{"role": "user", "content": chat_prompt}]
        
        result = await self._call_llm(
            model=self.model_filter,  # ä½¿ç”¨è½»é‡æ¨¡å‹
            messages=messages,
            temperature=0.8,
            timeout=60.0
        )
        
        if result:
            logger.info(f"âœ… èŠå¤©å›å¤ç”ŸæˆæˆåŠŸ")
            return {
                "success": True,
                "mode": "chat",
                "response": result
            }
        else:
            # LLM è¶…æ—¶æˆ–å¤±è´¥ï¼Œè¿”å›é™çº§å›å¤
            logger.warning("èŠå¤© LLM è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é™çº§å›å¤")
            fallback_response = "æˆ‘ç°åœ¨æœ‰ç‚¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚æˆ–è€…ä½ å¯ä»¥å‘Šè¯‰æˆ‘å…·ä½“çš„æ§åˆ¶æŒ‡ä»¤ï¼Œæ¯”å¦‚'å¤ä½'ã€'å‘å·¦ç§»åŠ¨'ç­‰ã€‚"
            return {
                "success": True,  # æ”¹ä¸º Trueï¼Œé¿å…å‰ç«¯æ˜¾ç¤ºé”™è¯¯
                "mode": "chat",
                "response": fallback_response,
                "fallback": True  # æ ‡è®°ä¸ºé™çº§å“åº”
            }
    
    async def handle_work(
        self, 
        user_message: str, 
        skills_description: str,
        current_angles: Optional[list] = None
    ) -> Dict[str, Any]:
        """
        ã€å·¥ä½œæ¨¡å¼ã€‘ä½¿ç”¨ DeepSeek å¤„ç†å·¥ä½œæŒ‡ä»¤
        
        Args:
            user_message: ç”¨æˆ·æŒ‡ä»¤
            skills_description: æŠ€èƒ½æè¿°æ–‡æ¡£
            current_angles: å½“å‰å…³èŠ‚è§’åº¦
        
        Returns:
            åŒ…å« skill å’Œ args çš„ç»“æœ
        """
        logger.info("âš™ï¸ [Work Mode] ä½¿ç”¨ DeepSeek å¤„ç†å·¥ä½œæŒ‡ä»¤...")

        # [Fast Path] å¸¸è§æŒ‡ä»¤å¿«é€Ÿé€šé“ï¼Œé¿å… LLM è¶…æ—¶
        fast_commands = {
            "ç‚¹å¤´": {"skill": "perform_action", "args": {"action_name": "nod"}, "response": "å¥½çš„ï¼Œæ‰§è¡Œç‚¹å¤´åŠ¨ä½œ"},
            "nod": {"skill": "perform_action", "args": {"action_name": "nod"}, "response": "OK, nodding"},
            "æŒ¥æ‰‹": {"skill": "perform_action", "args": {"action_name": "wave"}, "response": "å¥½çš„ï¼Œå‘å¤§å®¶æŒ¥æ‰‹"},
            "wave": {"skill": "perform_action", "args": {"action_name": "wave"}, "response": "OK, waving"},
            "å¤ä½": {"skill": "apply_preset", "args": {"name": "home"}, "response": "æ­£åœ¨å¤ä½æœºæ¢°è‡‚"},
            "reset": {"skill": "apply_preset", "args": {"name": "home"}, "response": "Resetting robot"},
            "å›é›¶": {"skill": "apply_preset", "args": {"name": "home"}, "response": "æ­£åœ¨å›é›¶"},
            "è·³èˆ": {"skill": "perform_action", "args": {"action_name": "dance"}, "response": "Music! å¼€å§‹è·³èˆï¼"},
            "è½¬åœˆ": {"skill": "perform_action", "args": {"action_name": "spin"}, "response": "å¼€å§‹æ—‹è½¬"},
        }

        for key, cmd in fast_commands.items():
            if key in user_message or (key.lower() in user_message.lower()):
                logger.info(f"âš¡ [Fast Path] Work command detected: {key}")
                # æ³¨å…¥ current_angles å¦‚æœéœ€è¦
                if current_angles and "args" in cmd:
                    cmd["args"]["current_angles"] = current_angles
                return {
                    "success": True,
                    "mode": "work",
                    **cmd
                }
        
        system_prompt = f"""ä½ æ˜¯æœºæ¢°è‡‚åŠ©æ‰‹Zeroã€‚
{skills_description}

## ä»»åŠ¡:
è¯·æ ¹æ®ç”¨æˆ·æŒ‡ä»¤é€‰æ‹©åˆé€‚çš„å·¥å…·(Skill)æ¥æ§åˆ¶æœºæ¢°è‡‚ã€‚

## å“åº”æ ¼å¼ (JSON):
å¿…é¡»è¿”å›æ ‡å‡†çš„ JSON æ ¼å¼ï¼š
{{
    "mode": "work",
    "response": "ç»™ç”¨æˆ·çš„å›å¤",
    "skill": "è¦è°ƒç”¨çš„å‡½æ•°å",
    "args": {{ "å‚æ•°å": å€¼ }}
}}

## ç¤ºä¾‹:
- ç”¨æˆ·: "åŸºåº§è½¬åˆ°90åº¦"
  å“åº”: {{"mode": "work", "response": "å¥½çš„ï¼Œæ­£åœ¨è°ƒæ•´åŸºåº§", "skill": "control_joint", "args": {{"joint_index": 1, "angle": 90}}}}
- ç”¨æˆ·: "å¤ä½"
  å“åº”: {{"mode": "work", "response": "æ­£åœ¨å¤ä½", "skill": "apply_preset", "args": {{"name": "home"}}}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        result = await self._call_llm(
            model=self.model_decision,  # ä½¿ç”¨ DeepSeek
            messages=messages,
            temperature=0.7,
            timeout=90.0  # å…è®¸æ›´é•¿æ—¶é—´
        )
        
        if result:
            try:
                # æ¸…ç† JSON
                clean_result = result.replace("```json", "").replace("```", "").strip()
                parsed = json.loads(clean_result)
                
                logger.info(f"âœ… å·¥ä½œæŒ‡ä»¤è§£ææˆåŠŸ: {parsed.get('skill')}")
                
                # æ³¨å…¥å½“å‰è§’åº¦
                if current_angles and "args" in parsed:
                    parsed["args"]["current_angles"] = current_angles
                
                return {
                    "success": True,
                    **parsed
                }
            except json.JSONDecodeError as e:
                logger.error(f"JSON è§£æå¤±è´¥: {e}, åŸå§‹å“åº”: {result}")
                return {
                    "success": False,
                    "error": "å·¥ä½œæŒ‡ä»¤æ ¼å¼é”™è¯¯"
                }
        else:
            return {
                "success": False,
                "error": "å·¥ä½œæŒ‡ä»¤å¤„ç†å¤±è´¥"
            }
    
    async def handle_vision(
        self, 
        user_message: str, 
        image_data: Optional[str] = None,
        vision_context: str = ""
    ) -> Dict[str, Any]:
        """
        ã€è§†è§‰æ¨¡å¼ã€‘ä½¿ç”¨ MODEL_VISION å¤„ç†è§†è§‰ä»»åŠ¡
        
        Args:
            user_message: ç”¨æˆ·æŒ‡ä»¤
            image_data: Base64 ç¼–ç çš„å›¾åƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            vision_context: è§†è§‰æ£€æµ‹ç»“æœæè¿°ï¼ˆå¦‚"æ£€æµ‹åˆ°: äºº, æ¯å­"ï¼‰
        
        Returns:
            è§†è§‰ç†è§£ç»“æœ
        """
        logger.info(f"ğŸ‘ï¸ [Vision Mode] å¤„ç†è§†è§‰ä»»åŠ¡... ä¸Šä¸‹æ–‡: {vision_context}")
        
        if not self.model_vision:
            # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„è§†è§‰æ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨å†³ç­–æ¨¡å‹
            model_to_use = self.model_decision
        else:
            model_to_use = self.model_vision
            
        # æ„å»º Prompt
        system_prompt = f"""ä½ æ˜¯æœºæ¢°è‡‚åŠ©æ‰‹Zeroï¼Œæ‹¥æœ‰è§†è§‰èƒ½åŠ›ã€‚
å½“å‰è§†è§‰ä¼ æ„Ÿå™¨æ£€æµ‹åˆ°çš„ç‰©ä½“: {vision_context if vision_context else "æœªæ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“ï¼ˆæˆ–è€…æ˜¯æ‘„åƒå¤´æœªè¿æ¥ï¼‰"}ã€‚

ä»»åŠ¡: æ ¹æ®è§†è§‰ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœç”¨æˆ·é—®"ä½ åœ¨çœ‹ä»€ä¹ˆ"æˆ–"æœ‰ä»€ä¹ˆ"ï¼Œè¯·æ ¹æ®æ£€æµ‹ç»“æœå›ç­”ã€‚
"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        result = await self._call_llm(
            model=model_to_use,
            messages=messages,
            temperature=0.7,
            timeout=45.0
        )
        
        if result:
            return {
                "success": True,
                "mode": "vision",
                "response": result
            }
        else:
            return {
                "success": True, 
                "mode": "vision",
                "response": f"æˆ‘çœ‹ä¸å¤ªæ¸…... ä½†æˆ‘æ£€æµ‹åˆ°äº†: {vision_context}" if vision_context else "å¯¹ä¸èµ·ï¼Œæˆ‘çš„è§†è§‰ä¼ æ„Ÿå™¨ä¼¼ä¹æ²¡æœ‰è¿æ¥å¥½ï¼Œæˆ‘çœ‹ä¸åˆ°ç”»é¢ã€‚"
            }
    
    async def get_embedding(self, text: str) -> Optional[list]:
        """
        è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤ºï¼ˆç”¨äºä¸Šä¸‹æ–‡æ£€ç´¢ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
        
        Returns:
            å‘é‡åˆ—è¡¨ï¼Œå¤±è´¥è¿”å› None
        """
        if not self.model_embedding:
            logger.warning("æœªé…ç½® MODEL_EMBEDDING")
            return None
        
        # TODO: å®ç° Embedding API è°ƒç”¨
        # é€šå¸¸éœ€è¦ä¸åŒçš„ API endpoint
        logger.info("ğŸ”— Embedding åŠŸèƒ½å¾…å®ç°")
        return None
